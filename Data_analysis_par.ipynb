{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils Functions (to import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_joint_prob_distr(target, source_var1, source_var2, source_var3):\n",
    "    \n",
    "    count = len(source_var1)\n",
    "\n",
    "    # compute probabilities from (multi-dim) histogram frequencies\n",
    "    result, _ = np.histogramdd(\n",
    "        np.vstack([source_var1, source_var2, source_var3, target]).T, \n",
    "        bins=[np.max(source_var1), np.max(source_var2), np.max(source_var3), np.max(target)]\n",
    "    )\n",
    "    \n",
    "    return result / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SUI(joint_prob_distr):\n",
    "\n",
    "    # get dimensions\n",
    "    dim_x_past = joint_prob_distr.shape[0]\n",
    "    dim_y_pres = joint_prob_distr.shape[1]\n",
    "    dim_y_past = joint_prob_distr.shape[2]\n",
    "    dim_s = joint_prob_distr.shape[3]\n",
    "\n",
    "    # initialize arrays\n",
    "    spec_surprise_x = np.zeros(dim_s)\n",
    "    spec_surprise_y = np.zeros(dim_s)\n",
    "    spec_surprise_y_past = np.zeros(dim_s)\n",
    "\n",
    "    # compute specific information provided by each source variable about s (target)\n",
    "    for s in range(dim_s):\n",
    "\n",
    "        # p(s)\n",
    "        ps = np.sum(joint_prob_distr[:, :, :, s]) \n",
    "\n",
    "        # info provided by x past\n",
    "        for x in range(dim_x_past):\n",
    "            psx = np.sum(joint_prob_distr[x, :, :, s]) / (np.sum(joint_prob_distr[x, :, :, :]) + np.finfo(float).eps)\n",
    "            pxs = np.sum(joint_prob_distr[x, :, :, s]) / (np.sum(joint_prob_distr[:, :, :, s]) + np.finfo(float).eps)\n",
    "\n",
    "            spec_surprise_x[s] += pxs * (np.log2(1/(ps + np.finfo(float).eps)) - np.log2(1/(psx + np.finfo(float).eps)))\n",
    "\n",
    "        # info provided by y\n",
    "        for y in range(dim_y_pres):\n",
    "            psy = np.sum(joint_prob_distr[:, y, :, s]) / (np.sum(joint_prob_distr[:, y, :, :]) + np.finfo(float).eps)\n",
    "            pys = np.sum(joint_prob_distr[:, y, :, s]) / (np.sum(joint_prob_distr[:, :, :, s]) + np.finfo(float).eps)\n",
    "            \n",
    "            spec_surprise_y[s] += pys * (np.log2(1/(ps + np.finfo(float).eps)) - np.log2(1/(psy + np.finfo(float).eps)))\n",
    "\n",
    "        # info provided by y past\n",
    "        for y in range(dim_y_past):\n",
    "            psy = np.sum(joint_prob_distr[:, :, y, s]) / (np.sum(joint_prob_distr[:, :, y, :]) + np.finfo(float).eps)\n",
    "            pys = np.sum(joint_prob_distr[:, :, y, s]) / (np.sum(joint_prob_distr[:, :, :, s]) + np.finfo(float).eps)\n",
    "            \n",
    "            spec_surprise_y_past[s] += pys * (np.log2(1/(ps + np.finfo(float).eps)) - np.log2(1/(psy + np.finfo(float).eps)))\n",
    "\n",
    "    # compute IMin\n",
    "\n",
    "    IMin_x_y_ypast = 0\n",
    "    IMin_x_y = 0\n",
    "\n",
    "    for s in range(dim_s):\n",
    "        IMin_x_y_ypast += np.sum(joint_prob_distr[:, :, :, s]) * min(spec_surprise_x[s], spec_surprise_y[s], spec_surprise_y_past[s])\n",
    "        IMin_x_y += np.sum(joint_prob_distr[:, :, :, s]) * min(spec_surprise_x[s], spec_surprise_y[s])\n",
    "\n",
    "    return IMin_x_y - IMin_x_y_ypast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TE(joint_prob_distr):\n",
    "\n",
    "    p_ypast = np.sum(joint_prob_distr, axis=(0, 1, 3))\n",
    "    p_x_ypast = np.sum(joint_prob_distr, axis=(1, 3))\n",
    "    p_y_ypast = np.sum(joint_prob_distr, axis=(0, 3))\n",
    "    p_x_y_ypast = np.sum(joint_prob_distr, axis=3)\n",
    "    \n",
    "    def entropy(p):\n",
    "        p_nonzero = p[p > 0]  # Avoid log of zero\n",
    "        return - np.sum(p_nonzero * np.log2(p_nonzero))\n",
    "    \n",
    "    h_ypast = entropy(p_ypast)\n",
    "    h_x_ypast = entropy(p_x_ypast)\n",
    "    h_y_ypast = entropy(p_y_ypast)\n",
    "    h_x_y_ypast = entropy(p_x_y_ypast)\n",
    "    \n",
    "    return h_y_ypast - h_ypast - h_x_y_ypast + h_x_ypast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_DFI(joint_prob_distr):\n",
    "    \n",
    "    # marginal distributions\n",
    "    prob_ypast = np.sum(joint_prob_distr, axis=(0, 1, 3))\n",
    "    prob_x_ypast = np.sum(joint_prob_distr, axis=(1, 3))\n",
    "    prob_y_ypast = np.sum(joint_prob_distr, axis=(0, 3))\n",
    "    prob_ypast_s = np.sum(joint_prob_distr, axis=(0, 1))\n",
    "    prob_x_y_ypast = np.sum(joint_prob_distr, axis=3)\n",
    "    prob_y_ypast_s = np.sum(joint_prob_distr, axis=0)\n",
    "    prob_x_ypast_s = np.sum(joint_prob_distr, axis=1)\n",
    "    \n",
    "    def get_entropy(prob_dist):\n",
    "        prob_nonzero = prob_dist[prob_dist > 0]  # Filter out zero values\n",
    "        return -np.sum(prob_nonzero * np.log2(prob_nonzero))\n",
    "    \n",
    "    # entropies\n",
    "    h_ypast = get_entropy(prob_ypast)\n",
    "    h_x_ypast = get_entropy(prob_x_ypast)\n",
    "    h_y_ypast = get_entropy(prob_y_ypast)\n",
    "    h_ypast_s = get_entropy(prob_ypast_s)\n",
    "    h_x_y_ypast = get_entropy(prob_x_y_ypast)\n",
    "    h_y_ypast_s = get_entropy(prob_y_ypast_s)\n",
    "    h_x_ypast_s = get_entropy(prob_x_ypast_s)\n",
    "    h_x_y_ypast_s = get_entropy(joint_prob_distr)\n",
    "    \n",
    "    # compute DFI\n",
    "    dfi = h_y_ypast - h_ypast - h_x_y_ypast + h_x_ypast - h_y_ypast_s + h_ypast_s + h_x_y_ypast_s - h_x_ypast_s\n",
    "    \n",
    "    return dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_FIT_TE_DFI(feature, X, Y, hY, xtrap=20):\n",
    "    # Build the two four-variables probability distributions needed to compute FIT\n",
    "    pXYhYS = get_joint_prob_distr(feature, X, Y, hY)    # probability distribution for the PID with (Xp, Yp, Yt) as sources and S as target\n",
    "    pXShYY = get_joint_prob_distr(Y, X, feature, hY)    # probability distribution for the PID with (Xp, Yp, S) as sources and Yt as target\n",
    "\n",
    "    # Compute the two FIT atoms and FIT\n",
    "    sui_S = get_SUI(pXYhYS)\n",
    "    sui_Y = get_SUI(pXShYY)\n",
    "\n",
    "    fit = np.min([sui_S, sui_Y])\n",
    "\n",
    "    # Compute TE\n",
    "    te = compute_TE(pXYhYS)\n",
    "\n",
    "    # Compute DFI\n",
    "    dfi = compute_DFI(pXYhYS)\n",
    "\n",
    "    # Compute quadratic extrapolation bias correction for FIT and TE\n",
    "    fit_all = fit\n",
    "    te_all = te\n",
    "\n",
    "    FIT2 = np.zeros(xtrap)\n",
    "    FIT4 = np.zeros(xtrap)\n",
    "    TE2 = np.zeros(xtrap)\n",
    "    TE4 = np.zeros(xtrap)\n",
    "\n",
    "    for xIdx in range(xtrap):\n",
    "\n",
    "        numberOfTrials = len(X)\n",
    "\n",
    "        # Shuffled indexes in 0,ntrials range\n",
    "        rIdx = npr.choice(numberOfTrials, numberOfTrials, replace=False)\n",
    "        # Divide the indexes in 2 and 4 parts\n",
    "        idx2 = np.array_split(rIdx, 2) \n",
    "        idx4 = np.array_split(rIdx, 4)\n",
    "        \n",
    "        # Stack all the sources in data, separate into 2 and 4 parts, and distinguish between s and y targets\n",
    "        data = np.stack(np.array([feature, X, Y, hY]),axis=1)\n",
    "        data2_s = np.stack(np.array([data[idx2[i]] for i in range(2)]), axis = 0)\n",
    "        \n",
    "\n",
    "        data2_y = data2_s[:, :, [2, 1, 0, 3]]\n",
    "        data2_tot = np.stack(np.array([data2_s,data2_y]), axis=0)\n",
    "        \n",
    "        data4_s = np.stack(np.array([data[idx4[i]] for i in range(4)]), axis = 0)\n",
    "        data4_y = data4_s[:, :, [2, 1, 0, 3]]\n",
    "        data4_tot = np.stack(np.array([data4_s,data4_y]), axis=0)\n",
    "        \n",
    "        # Compute Joint, SUI, FIT and TE for the 2 divided version\n",
    "        joint2 = [[\n",
    "            get_joint_prob_distr(*[data2_tot[ch,row, :, i] for i in range(4)])\n",
    "            for row in range(data2_tot.shape[1])]\n",
    "            for ch in range(data2_tot.shape[0])\n",
    "        ]\n",
    "        \n",
    "        SUI_2 = [[get_SUI(joint2[ch][i]) for i in range(2)] for ch in range(len(joint2))]\n",
    "        FIT2[xIdx] = np.mean(np.min(SUI_2,axis=0))\n",
    "        TE2[xIdx] = np.mean([compute_TE(joint2[0][i]) for i in range(2)])\n",
    "        \n",
    "        # Compute Joint, SUI, FIT and TE for the 4 divided version\n",
    "        joint4 = [[\n",
    "            get_joint_prob_distr(*[data4_tot[ch,row, :, i] for i in range(4)])\n",
    "            for row in range(data4_tot.shape[1])]\n",
    "            for ch in range(data4_tot.shape[0])\n",
    "        ]\n",
    "\n",
    "        \n",
    "        SUI_4 = [[get_SUI(joint4[ch][i]) for i in range(4)] for ch in range(len(joint4))]\n",
    "        FIT4[xIdx] = np.mean(np.min(SUI_4,axis=0))\n",
    "        TE4[xIdx] = np.mean([compute_TE(joint4[0][i]) for i in range(4)])\n",
    "\n",
    "\n",
    "    # Compute the linear and quadratic interpolations for FIT and TE\n",
    "\n",
    "    x = [1/len(idx2[0]), 1/len(idx4[0]), 1/len(rIdx)]\n",
    "    y = [np.mean(FIT4), np.mean(FIT2), fit_all]\n",
    "\n",
    "    p2 = np.polyfit(x, y, 2)\n",
    "    p1 = np.polyfit(x, y, 1) \n",
    "    FITQe = p2[2]\n",
    "    FITLe = p1[1]\n",
    "         \n",
    "    y = [np.mean(TE4), np.mean(TE2), te_all]\n",
    "    \n",
    "    p2 = np.polyfit(x, y, 2)\n",
    "    p1 = np.polyfit(x, y, 1) \n",
    "    TEQe = p2[2]\n",
    "    TELe = p1[1]\n",
    "\n",
    "    return te, dfi, fit # , TEQe, TELe, FITQe, FITLe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_delay = 120\n",
    "bins = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = 'data/data/metadata.mat'\n",
    "metadata = h5.File(meta_path,'r')\n",
    "\n",
    "timesteps = len(metadata['time']) # Number of timesteps\n",
    "num_files = int(np.array(metadata['Ns']).item()) # Number of files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 300 # timesteps to compute fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_fit = np.full((num_files, time, max_delay), np.nan)\n",
    "left_di = left_fit.copy()\n",
    "right_fit = left_fit.copy()\n",
    "right_di = left_fit.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_cycle (temp, d):\n",
    "    t = temp + max_delay + 1\n",
    "    # Discretize Neural Signals\n",
    "    _, bin_edges = pd.qcut(leeg[t,:], bins, retbins=True)\n",
    "    leeg_d = np.digitize(leeg[t,:], bins=bin_edges, right=False)\n",
    "    _, bin_edges = pd.qcut(ldeeg[t,:], bins, retbins=True)\n",
    "    ldeeg_d = np.digitize(ldeeg[t,:], bins=bin_edges, right=False)\n",
    "    _, bin_edges = pd.qcut(leeg[t-d,:], bins, retbins=True)\n",
    "    leegp = np.digitize(leeg[t-d,:], bins=bin_edges, right=False)\n",
    "    _, bin_edges = pd.qcut(ldeeg[t-d,:], bins, retbins=True)\n",
    "    ldeegp = np.digitize(ldeeg[t-d,:], bins=bin_edges, right=False)\n",
    "\n",
    "    _, bin_edges = pd.qcut(reeg[t,:], bins, retbins=True)\n",
    "    reeg_d = np.digitize(reeg[t,:], bins=bin_edges, right=False)\n",
    "    _, bin_edges = pd.qcut(rdeeg[t,:], bins, retbins=True)\n",
    "    rdeeg_d = np.digitize(rdeeg[t,:], bins=bin_edges, right=False)\n",
    "    _, bin_edges = pd.qcut(reeg[t-d,:], bins, retbins=True)\n",
    "    reegp = np.digitize(reeg[t-d,:], bins=bin_edges, right=False)\n",
    "    _, bin_edges = pd.qcut(rdeeg[t-d,:], bins, retbins=True)\n",
    "    rdeegp = np.digitize(rdeeg[t-d,:], bins=bin_edges, right=False)\n",
    "\n",
    "    # Left eye visibility, Right to left tranfer\n",
    "    pastX = (reegp - 1) * bins + rdeegp\n",
    "    Y = (leeg_d - 1) * bins + ldeeg_d\n",
    "    pastY = (leegp - 1) * bins + ldeegp\n",
    "\n",
    "    # Right Eye visibility, Left to Right transfer\n",
    "    pastXR = (leegp - 1) * bins + ldeegp\n",
    "    YR = (reeg_d - 1) * bins + rdeeg_d\n",
    "    pastYR = (reegp - 1) * bins + rdeegp\n",
    "                 \n",
    "    right_di, _, right_fit = compute_FIT_TE_DFI(RES, pastXR, YR, pastYR)\n",
    "                                \n",
    "    left_di, _, left_fit = compute_FIT_TE_DFI(LES, pastX, Y, pastY)\n",
    "\n",
    "    return temp, d, right_fit, right_di, left_fit, left_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   24.6s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:   28.6s\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of 100 | elapsed:   32.7s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   32.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of 100 | elapsed:   35.1s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   35.3s finished\n"
     ]
    }
   ],
   "source": [
    "for file in range(num_files):\n",
    "    print('File ',file+1)\n",
    "    file_path = 'data/data/data_s{0}.mat'.format(file+1)\n",
    "    data = h5.File(file_path,'r')\n",
    "\n",
    "    eeg_prel = np.array(data['feeg']) # eeg data\n",
    "    eye_visib_prel = np.array(data['eyebubs'])\n",
    "    ntrials = np.shape(eeg_prel)[2]\n",
    "\n",
    "    if ((np.shape(eeg_prel)[2] - 1) %4==0):\n",
    "        ntrials = np.shape(eeg_prel)[2] - 1\n",
    "    elif ((np.shape(eeg_prel)[2] - 2) %4==0):\n",
    "        ntrials = np.shape(eeg_prel)[2] - 2\n",
    "    elif ((np.shape(eeg_prel)[2] - 3) %4==0):\n",
    "        ntrials = np.shape(eeg_prel)[2] - 3\n",
    "\n",
    "    eeg = eeg_prel[:,:,:ntrials]\n",
    "    eye_visib = eye_visib_prel[:,:ntrials] # Eye region visibility, related to stimulus\n",
    "\n",
    "    left_electrode = np.array(data['LEmaxMI']).item() # Left electrode with max MI\n",
    "    right_electrode = np.array(data['REmaxMI']).item() # Right electrode with max MI\n",
    "    leeg = eeg[int(left_electrode-1),:,:]\n",
    "    reeg = eeg[int(right_electrode-1),:,:]\n",
    "\n",
    "    left_eye_v = eye_visib[0,:]\n",
    "    right_eye_v = eye_visib[1,:]\n",
    "\n",
    "\n",
    "    # Discretize stimulus\n",
    "    _, bin_edges = pd.qcut(left_eye_v, bins, retbins=True)\n",
    "    LES = np.digitize(left_eye_v, bins=bin_edges, right=False)\n",
    "    _, bin_edges = pd.qcut(right_eye_v, bins, retbins=True)\n",
    "    RES = np.digitize(right_eye_v, bins=bin_edges, right=False)\n",
    "\n",
    "    # Computing temporal derivatives of eeg\n",
    "    ldeeg = np.full(np.shape(leeg), np.nan)\n",
    "    ldeeg[1:timesteps,:] = leeg[1:timesteps,:] - leeg[0:(timesteps-1),:]\n",
    "    rdeeg = np.full(np.shape(reeg), np.nan)\n",
    "    rdeeg[1:timesteps,:] = reeg[1:timesteps,:] - reeg[0:(timesteps-1),:]\n",
    "\n",
    "    index_iter = product(range(time), range(max_delay))\n",
    "    results = Parallel(n_jobs=-1,verbose=10)(\n",
    "    delayed(inner_cycle)(*pair) for pair in index_iter\n",
    "    )\n",
    "    \n",
    "    for res in results:\n",
    "        t, d, fitr, ter, fitl, tel = res\n",
    "        right_di[file, t, d] = ter\n",
    "        right_fit[file, t, d] = fitr\n",
    "        left_di[file, t, d] = tel\n",
    "        left_fit[file, t, d] = fitl\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
